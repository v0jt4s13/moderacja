{% extends "base.html" %}
    
{% block title %}Manual{% endblock %}

{% block extra_styles %}

  <style>
    /* body {
      font-family: Arial, sans-serif;
      line-height: 1.6;
      padding: 20px;
      max-width: 800px;
      margin: auto;
    } */
    .rozwin {
      cursor: pointer;
      /* background-color: #f0f0f0; */
      padding: 10px;
      margin-top: 20px;
      border-left: 5px solid #007bff;
    }
    .hidden {
      display: none;
      padding: 10px;
      /* background-color: #fafafa; */
      border: 1px solid #ccc;
      margin-top: 5px;
    }
    .pre-code pre {
        background-color: #eeeeee24;
        padding: 10px;
        overflow-x: auto;
        margin-left: 1em;
    }
    .pre-code {
        text-indent: 0.1em;
    }
    img {
      max-width: 100%;
      margin: 10px 0;
      border: 1px solid #ccc;
    }
    p {
       line-height: 1.6; 
       text-indent: 1em;
    }
    ol {
      list-style: decimal;
      display: list-item;
      padding-inline-start: 0px;
    }
    ol li {
      list-style: decimal;
      display: list-item;
      padding-inline-start: 0px;
    }
    ul {
      list-style: none;
      padding-inline-start: 0.4em;
      display: flex;
      flex-direction: column;
    }
    ul li {
      list-style: disc;
      display: list-item;
    }
    .code-text {
      font-family:;
      line-height: normal;
      background:rgba(135,131,120,.15);
      color:#EB5757;
      border-radius:4px;
      font-size:85%;
      padding:0.2em 0.4em;
      position:relative;
      bottom:0.065em;
    }
  </style>

{% endblock %}

{% block header %}Manual{% endblock %}

{% block content %}

  <h1>Docker, Ollama, OpenWebUI - Instalacja i konfiguracja</h1>

  <h2>1. Docker</h2>
  <p>Docker to krytyczny element naszej architektury. Dlaczego? DziÄ™ki Dockerowi uruchomimy kontenery wewnÄ…trz ktÃ³rych bÄ™dzie dziaÅ‚aÄ‡ infrastruktura zwiÄ…zana z chatem.</p>
  <p>PoniÅ¼ej znajdziesz info dotyczÄ…ce instalacji na rÃ³Å¼nych systemach operacyjnych.</p>

  <div class="rozwin">### Korzystasz z Windows?</div>
  <div class="hidden">
    <p><strong>1. Virtual Machine Platform (WSL2)</strong></p>
    <p>PierwszÄ… rzeczÄ…, ktÃ³rÄ… trzeba zrobiÄ‡, jest wÅ‚Ä…czenie funkcji Windows o nazwie Platforma maszyny wirtualnej (Virtual Machine Platform). Instaluje ona tak zwany WSL2, czyli Windows Subsystem for Linux 2, ktÃ³ry dziaÅ‚a z systemem Windows 11 oraz Windows 10 w wersji 1903 build 18362 lub nowszej.</p>
    <p>Aby zainstalowaÄ‡ WSL2, naciÅ›nij przycisk Windows lub Start i wpisz "Funkcje Systemu Windows" (Windows Features). NastÄ™pnie przewiÅ„ w dÃ³Å‚, aÅ¼ zobaczysz "Platforma Maszyny Wirtualnej" (Virtual Machine Platform). Zaznacz to pole i kliknij OK. Spowoduje to zastosowanie tej funkcji w systemie Windows, a nastÄ™pnie pojawi siÄ™ proÅ›ba o ponowne uruchomienie komputera.</p>
    <img src="/static/webutils/img/vmp_wsl2.webp" alt="Platforma Maszyny Wirtualnej">
    <p>Po ponownym uruchomieniu komputera, przejdÅº do kolejnego kroku.</p>

    <p><strong>2. Instalacja Dockera - Windows lub MacOS</strong></p>
    <p>JeÅ›li korzystasz z Mac lub Windows - przejdÅº na stronÄ™ <a href="https://docs.docker.com/get-started/get-docker/">ğŸ”— https://docs.docker.com/get-started/get-docker/</a> i wybierz wersjÄ™ dla Twojego systemu operacyjnego.</p>
    <p>Po zakoÅ„czeniu instalacji, <strong>uruchom ponownie komputer</strong> ...</p>
  </div>

  <div class="rozwin">### Korzystasz z Mac?</div>
  <div class="hidden">
    <p>Aby zainstalowaÄ‡ Docker Desktop na systemie MacOS, postÄ™puj wedÅ‚ug poniÅ¼szych krokÃ³w:</p>
    <div style="margin-left:3em">
      <ol>
        <li>PrzejdÅº do oficjalnej&nbsp;<a href="https://docs.docker.com/desktop/setup/install/mac-install/"> strony Docker</a></li>
        <li>Pobierz instalator dla Twojego procesora (Intel lub ARM64)</li>
        <li>OtwÃ³rz .dmg i przeciÄ…gnij aplikacjÄ™ Docker do Aplikacji</li>
        <li>Uruchom i dokoÅ„cz konfiguracjÄ™</li>
      </ol>
    </div>
  </div>

  <div class="rozwin">### Korzystasz z Linux?</div>
  <div class="hidden">
    <p>JeÅ›li korzystasz z Linuksa, masz dwie opcje:</p>
    <div style="margin-left:3em">
      <ol>
        <li>Instalacja Dockera przez terminal:&nbsp;<a href="https://docs.docker.com/engine/install/">https://docs.docker.com/engine/install/</a></li>
        <li>Albo Docker Desktop:&nbsp;<a href="https://docs.docker.com/get-started/get-docker/">https://docs.docker.com/get-started/get-docker/</a></li>
      </ol>
    </div>
  </div>

  <div>
    <p>Aby sprawdziÄ‡, czy wszystko dziaÅ‚a poprawnie, otwÃ³rz terminal:</p>
    <div style="margin-left:3em">
      <ol>
        <li>
          <div class="pre-code">
            Sprawdzenie wersji dockera<br>
            <pre><code>docker --version</code></pre>
          </div>
        </li><li>
          <div class="pre-code">
            Uruchomienie przykÅ‚adowego kontenera<br>
            <pre><code>docker run -d --name nginx nginx</code></pre>
          </div>
        </li><li>
          <div class="pre-code">
            Sprawdzenie czy dziala<br>
            <pre><code>docker ps</code></pre>
          </div>
        </li><li>
          <div class="pre-code">
            UsuniÄ™cie konteneru z dockera<br>
            <pre><code>docker rm nginx --force</code></pre>
          </div>
        </li>
      </ol>
    </div>
  </div> 
  
  <h2>2. Ollama</h2>
  <p>Ollama sÅ‚uÅ¼y do uruchamiania lokalnie modeli jÄ™zykowych (LLM). To daemon, ktÃ³ry stawia lokalny backend LLM-Ã³w i â€rozmawiaâ€ po HTTP niemal 1-do-1 z tym, co zwraca endpoint. 
    Z pomocÄ… Ollama uruchomisz wiele otwartych modeli LLM - od Llamy 3 i Mistrala po Qwena czy GemmÄ™. 
    Wszystko dzieje siÄ™ lokalnie, co daje wiÄ™kszÄ… prywatnoÅ›Ä‡ niÅ¼ korzystanie z modeli firm trzecich. 
    NajwiÄ™kszym plusem jest to, Å¼e uruchamiajÄ…c LLM lokalnie jedyne rachunki, jakie pÅ‚acisz, to prÄ…d i trochÄ™ miejsca na dysku.
  </p>

  <h2>3. OpenWebUI</h2>
  <p>OpenWebUI to frontend do lokalnych LLM-Ã³w oferujÄ…cy panel w przeglÄ…darce, konta uÅ¼ytkownikÃ³w, historiÄ™ czatÃ³w i moÅ¼liwoÅ›Ä‡ przeÅ‚Ä…czania backendÃ³w. (Ollama, OpenAI, model na Hugging Face Inference)
    A wszystko zamkniÄ™te w jednym kontenerze.
  </p>
  <p>
    <h3>Instalacja Ollama i OpenWebUI</h3>
    <div style="margin-left:3em">
      <ol>
        <li>
          <div class="pre-code">
            StwÃ³rz katalog <span class="code-text">aichat-locally</span> i stwÃ³rz w nim plik <span class="code-text">docker-compose.yml</span> o poniÅ¼szej zawartoÅ›ci:
            <pre><code>
services:
  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-0.6.9-rc0}

  open-webui:
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
    ports:
      - ${OPEN_WEBUI_PORT-3000}:8080
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY='
    # extra_hosts:
    #  - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  ollama: {}
  open-webui: {}
            </code></pre>
          </div>
        </li><li>
          <div class="pre-code">
            OtwÃ³rz terminal i przejdÅº do katalogu <span class="code-text">aichat-locally</span>, a nastÄ™pnie uÅ¼yj polecenia:
            <pre><code>docker compose up -d</code></pre>
          </div>
        </li><li>
          <div class="pre-code">
            Po zakoÅ„czeniu procesu tworzenia, przejdÅº do przeglÄ…darki na <a href="http://localhost:3000/" target="_blank">http://localhost:3000/</a>
          </div>
        </li><li>
          <div class="pre-code">
            Kliknij Get Started. ZaÅ‚Ã³Å¼ konto (dane bÄ™dÄ… zapisane lokalnie u Ciebie na komputerze)
          </div>
        </li>
      </ol>
    </div>
  </p>  

  <h2>4. Uruchomienie LLM lokalnie - jaki model wybraÄ‡?</h2>
  <p>1. <a href="https://ollama.com/library" target="_blank">https://ollama.com/library</a> znajdziesz bazÄ™ dostÄ™pnych modeli do uruchomienia lokalnie z poziomu OpenWebUI.</p>
  <p>2. <a href="https://github.com/ollama/ollama" target="_blank">https://github.com/ollama/ollama</a> wiÄ™cej przykÅ‚adowych modeli jakie moÅ¼na pobraÄ‡</p>
  
  <div style="margin-left:3em">
    <p style="margin-left:-2em">Przy wybieraniu modelu, zwrÃ³Ä‡ uwagÄ™ na:</p>
    <ul>
      <li>jego zastosowania</li>
      <li>rozmiar na dysku (Å¼eby nie pobieraÄ‡ np 800 GB)</li>
      <li>wsparcie dla GPU / CPU</li>
    </ul>
  </div>
  
  <p>3. Aby zainstalowaÄ‡ wybrany model, przejdÅº do OpenWebUI, kliknij w "Select a model", a nastÄ™pnie wpisz np.: <span class="code-text">gemma3</span></p>
  <p>Gdy model zostanie pobrany moÅ¼emy zaczÄ…Ä‡ rozmowÄ™ z chatem.</p>

  <h2>5. PodpiÄ™cie zewnÄ™trznych API (OpenAI, Claude, Gemini etc)</h2>
  <p>Aby podpiÄ…Ä‡ np. OpenAI API, zaloguj siÄ™ do dziaÅ‚ajÄ…cego OpenWebUI, kliknij ikonkÄ™ Twego avatara, nastÄ™pnie "Settings". 
    PrzejdÅº do Connections, nastÄ™pnie kliknij w ikonkÄ™ (+). WprowadÅº API URL <span class="code-text">https://api.openai.com/v1</span> oraz API Key i zapisz.
  </p>
  <p>Od tej chwili na liÅ›cie modeli powinieneÅ› mieÄ‡ modele z OpenAI.</p>

  <h2>6. WÅ‚asna Baza Wiedzy (RAG)</h2>
  <p>RAG (Retrieval-Augmented Generation) to technika, w ktÃ³rej model jÄ™zykowy najpierw wyszukuje pasujÄ…ce fragmenty z bazy dokumentÃ³w, a dopiero potem generuje odpowiedÅº, wplatajÄ…c te fragmenty jako kontekst. DziÄ™ki temu odpowiedzi sÄ… bardziej precyzyjne i oparte na realnych danych, a nie tylko na â€wiedzyâ€ modelu.</p>
  <p>OpenWebUI z wÅ‚Ä…czonym RAG-iem to proste poÅ‚Ä…czenie interfejsu www i wyszukiwania kontekstowego. Po dodaniu plikÃ³w (PDF, Markdown, strony WWW) moÅ¼esz zadawaÄ‡ pytania, a model odpowiada z uwzglÄ™dnieniem treÅ›ci znalezionych w tych dokumentach.</p>

  <h2>7. UdostÄ™pnienie Chata z Twojego komputera do internetu po HTTPS</h2>

  <script>
    document.querySelectorAll('.rozwin').forEach(button => {
      button.addEventListener('click', () => {
        const next = button.nextElementSibling;
        if (next && next.classList.contains('hidden')) {
          next.style.display = next.style.display === 'block' ? 'none' : 'block';
        }
      });
    });
  </script>

{% endblock %}